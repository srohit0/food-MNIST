{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "food_MNIST_keras_resnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srohit0/food_mnist/blob/master/examples/food_MNIST_keras_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBJkeXZBWLP5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# food MNIST\n",
        "\n",
        "\n",
        "<img src=https://raw.githubusercontent.com/srohit0/food_mnist/master/images/food-collage.jpg  width=\"600\" height=\"300\" align=\"center\">\n",
        "\n",
        "---\n",
        "food dataset is a challanging dataset for most networks. This notebook shows results of applying resnet on food-MNIST with and without augmentation. It is easy to see that accuracy increases with augmentation supporting deep learning principle of:\n",
        "### more data, better results\n",
        "---\n",
        "\n",
        "## 1. Clone dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay_rj2zYWRXQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzaG7swR8Dup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget -q https://raw.githubusercontent.com/raghakot/keras-resnet/master/resnet.py -O resnet.py\n",
        "! [ ! -d \"food_mnist\" ] &&  git clone -q https://github.com/srohit0/food_mnist.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRH72nY4XTng",
        "colab_type": "text"
      },
      "source": [
        "## 2. Adapt Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxAMdOHwLoj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randomize(dataset, labels):\n",
        "  permutation = np.random.permutation(labels.shape[0])\n",
        "  shuffled_dataset = dataset[permutation,:,:]\n",
        "  shuffled_labels = labels[permutation]\n",
        "  return shuffled_dataset, shuffled_labels\n",
        "\n",
        "\n",
        "\n",
        "def divide_dataset(dataset, labels):\n",
        "    train_pct = 0.80; \n",
        "    # divide dataset into training and validation set\n",
        "    train_index = int(dataset.shape[0]*train_pct)\n",
        "    t_X = dataset[:train_index, :]\n",
        "    t_Y = labels[:train_index]\n",
        "    v_X = dataset[train_index:,:]\n",
        "    v_Y = labels[train_index:]\n",
        "    \n",
        "    return (t_X, t_Y), (v_X, v_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d82Kj01T8mLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a941353-4973-486c-88c3-c80a958eb813"
      },
      "source": [
        "from __future__ import print_function\n",
        "import food_mnist\n",
        "import resnet\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "early_stopper = EarlyStopping(min_delta=0.001, patience=10)\n",
        "csv_logger = CSVLogger('resnet18_cifar10.csv')\n",
        "\n",
        "batch_size = 32\n",
        "nb_classes = 10\n",
        "epochs = 200\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols, img_channels= 32, 32, 3\n",
        "\n",
        "# The data, shuffled and split between train and test sets:\n",
        "(X_train, Y_train), (X_test, Y_test) = food_mnist.load_data(img_cols, img_rows)\n",
        "\n",
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "Y = np.concatenate((Y_train, Y_test), axis=0)\n",
        "\n",
        "X, Y = randomize(X, Y)\n",
        "(X_train, Y_train), (X_test, Y_test) = divide_dataset(X, Y)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(Y_test, nb_classes)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# subtract mean and normalize\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "X_train -= mean_image\n",
        "X_test -= mean_image\n",
        "X_train /= 128.\n",
        "X_test /= 128."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz0Tw6SjYKxG",
        "colab_type": "text"
      },
      "source": [
        "## 3. Create Deep Learning Model (ResNet-18)\n",
        "---\n",
        "\n",
        "\n",
        "<img src=https://www.researchgate.net/profile/Paolo_Napoletano/publication/322476121/figure/tbl1/AS:668726449946625@1536448218498/ResNet-18-Architecture_W640.jpg height=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MqpflWD8qay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "51960ec0-22d8-45bb-87ec-43d5c9f707ed"
      },
      "source": [
        "model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0622 00:04:03.561514 140196683085696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0622 00:04:03.582317 140196683085696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0622 00:04:03.585685 140196683085696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0622 00:04:03.613817 140196683085696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0622 00:04:03.614673 140196683085696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0622 00:04:04.317631 140196683085696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0622 00:04:04.382694 140196683085696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0622 00:04:05.885097 140196683085696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0622 00:04:05.907883 140196683085696 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaQt1hWzYoM2",
        "colab_type": "text"
      },
      "source": [
        "## 4. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOb5S-57AhEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Train(data_augmentation):\n",
        "  if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(X_test, Y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=[lr_reducer, early_stopper, csv_logger])\n",
        "  else:\n",
        "      print('Using real-time data augmentation.')\n",
        "      # This will do preprocessing and realtime data augmentation:\n",
        "      datagen = ImageDataGenerator(\n",
        "          featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "          samplewise_center=False,  # set each sample mean to 0\n",
        "          featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "          samplewise_std_normalization=False,  # divide each input by its std\n",
        "          zca_whitening=False,  # apply ZCA whitening\n",
        "          rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "          width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "          height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "          horizontal_flip=True,  # randomly flip images\n",
        "          vertical_flip=False)  # randomly flip images\n",
        "\n",
        "      # Compute quantities required for featurewise normalization\n",
        "      # (std, mean, and principal components if ZCA whitening is applied).\n",
        "      datagen.fit(X_train)\n",
        "\n",
        "      # Fit the model on the batches generated by datagen.flow().\n",
        "      model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
        "                          steps_per_epoch=X_train.shape[0] // batch_size,\n",
        "                          validation_data=(X_test, Y_test),\n",
        "                          epochs=epochs, verbose=1, max_q_size=1000,\n",
        "                          callbacks=[lr_reducer, early_stopper, csv_logger])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL2bfXGqYqeM",
        "colab_type": "text"
      },
      "source": [
        "### 4a. Train without augmentation \n",
        "\n",
        "1. Expected validation accuracy is around 40% with close to 100% training accurcy.\n",
        "2. Don't forget to observe overfitting symptoms\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXnzi28zMVhQ",
        "colab_type": "code",
        "outputId": "d8ac5e02-11c7-4da0-f2f7-a95a95bfb1e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "source": [
        "Train(data_augmentation=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not using data augmentation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0622 00:04:06.292895 140196683085696 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 4000 samples, validate on 1000 samples\n",
            "Epoch 1/200\n",
            "4000/4000 [==============================] - 11s 3ms/step - loss: 2.7331 - acc: 0.2860 - val_loss: 2.8066 - val_acc: 0.2980\n",
            "Epoch 2/200\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 2.3146 - acc: 0.3985 - val_loss: 2.5168 - val_acc: 0.3390\n",
            "Epoch 3/200\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 2.0972 - acc: 0.4645 - val_loss: 2.6300 - val_acc: 0.3220\n",
            "Epoch 4/200\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 1.8367 - acc: 0.5655 - val_loss: 2.7193 - val_acc: 0.3150\n",
            "Epoch 5/200\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 1.5696 - acc: 0.6510 - val_loss: 2.8232 - val_acc: 0.3210\n",
            "Epoch 6/200\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 1.2792 - acc: 0.7507 - val_loss: 3.2000 - val_acc: 0.3380\n",
            "Epoch 7/200\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 1.0691 - acc: 0.8317 - val_loss: 3.1813 - val_acc: 0.3430\n",
            "Epoch 8/200\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.7255 - acc: 0.9470 - val_loss: 2.8745 - val_acc: 0.3820\n",
            "Epoch 9/200\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.5850 - acc: 0.9933 - val_loss: 2.9434 - val_acc: 0.3850\n",
            "Epoch 10/200\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.5501 - acc: 0.9980 - val_loss: 2.9791 - val_acc: 0.4080\n",
            "Epoch 11/200\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.5268 - acc: 0.9998 - val_loss: 3.0269 - val_acc: 0.3940\n",
            "Epoch 12/200\n",
            "4000/4000 [==============================] - 4s 1ms/step - loss: 0.5104 - acc: 0.9998 - val_loss: 3.0678 - val_acc: 0.4060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOW1_d2SZJyW",
        "colab_type": "text"
      },
      "source": [
        "### 4b. Train with image augmentation \n",
        "\n",
        "1. Expected validation accuracy is around 50% with training accuracy around 75%.\n",
        "2. Overfitting is reduced with augmentation\n",
        "\n",
        "---\n",
        "**more data, better results**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etH5P9s_McEj",
        "colab_type": "code",
        "outputId": "92c995ff-6b26-412e-cd39-d2d83e670557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "source": [
        "Train(data_augmentation=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/200\n",
            "  3/125 [..............................] - ETA: 5s - loss: 2.0009 - acc: 0.6667"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=125, validation_data=(array([[[..., epochs=200, verbose=1, callbacks=[<keras.ca..., max_queue_size=1000)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 5s 41ms/step - loss: 2.1083 - acc: 0.5533 - val_loss: 2.7808 - val_acc: 0.4030\n",
            "Epoch 2/200\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 1.7500 - acc: 0.5915 - val_loss: 2.4465 - val_acc: 0.4040\n",
            "Epoch 3/200\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 1.6783 - acc: 0.6068 - val_loss: 2.2862 - val_acc: 0.4210\n",
            "Epoch 4/200\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 1.6238 - acc: 0.6322 - val_loss: 2.2268 - val_acc: 0.4310\n",
            "Epoch 5/200\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 1.5804 - acc: 0.6422 - val_loss: 2.2231 - val_acc: 0.4540\n",
            "Epoch 6/200\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 1.5723 - acc: 0.6420 - val_loss: 2.2024 - val_acc: 0.4350\n",
            "Epoch 7/200\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 1.5261 - acc: 0.6555 - val_loss: 2.1805 - val_acc: 0.4630\n",
            "Epoch 8/200\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 1.5162 - acc: 0.6607 - val_loss: 2.2127 - val_acc: 0.4560\n",
            "Epoch 9/200\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 1.4371 - acc: 0.6790 - val_loss: 2.1813 - val_acc: 0.4690\n",
            "Epoch 10/200\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 1.4339 - acc: 0.6808 - val_loss: 2.1712 - val_acc: 0.4600\n",
            "Epoch 11/200\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 1.3640 - acc: 0.7020 - val_loss: 2.2312 - val_acc: 0.4570\n",
            "Epoch 12/200\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 1.3944 - acc: 0.6950 - val_loss: 2.1924 - val_acc: 0.4630\n",
            "Epoch 13/200\n",
            "125/125 [==============================] - 5s 38ms/step - loss: 1.3504 - acc: 0.7065 - val_loss: 2.1996 - val_acc: 0.4630\n",
            "Epoch 14/200\n",
            "125/125 [==============================] - 5s 38ms/step - loss: 1.3106 - acc: 0.7208 - val_loss: 2.1863 - val_acc: 0.4840\n",
            "Epoch 15/200\n",
            "125/125 [==============================] - 5s 38ms/step - loss: 1.2822 - acc: 0.7350 - val_loss: 2.2026 - val_acc: 0.4730\n",
            "Epoch 16/200\n",
            "125/125 [==============================] - 5s 38ms/step - loss: 1.2218 - acc: 0.7548 - val_loss: 2.1723 - val_acc: 0.4710\n",
            "Epoch 17/200\n",
            "125/125 [==============================] - 5s 39ms/step - loss: 1.2056 - acc: 0.7588 - val_loss: 2.1791 - val_acc: 0.4780\n",
            "Epoch 18/200\n",
            "125/125 [==============================] - 5s 38ms/step - loss: 1.1754 - acc: 0.7652 - val_loss: 2.1861 - val_acc: 0.4770\n",
            "Epoch 19/200\n",
            "125/125 [==============================] - 5s 38ms/step - loss: 1.1998 - acc: 0.7552 - val_loss: 2.1863 - val_acc: 0.4780\n",
            "Epoch 20/200\n",
            "125/125 [==============================] - 5s 38ms/step - loss: 1.1660 - acc: 0.7668 - val_loss: 2.1835 - val_acc: 0.4840\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}